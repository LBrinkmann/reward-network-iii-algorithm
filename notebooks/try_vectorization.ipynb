{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy of environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sara Bonati\\Desktop\\MPI_work\\Machines\\Reward_network_task\\reward-network-iii-algorithm\\.venv\\lib\\site-packages\\seaborn\\rcmod.py:400: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "c:\\Users\\Sara Bonati\\Desktop\\MPI_work\\Machines\\Reward_network_task\\reward-network-iii-algorithm\\.venv\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from collections import Counter\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "def restructure_edges(network):\n",
    "    \"\"\"\n",
    "    This function restructures the edges from list of dicts\n",
    "    to one dict, to improve construction of edges matrix and \n",
    "    env vectorization\n",
    "    \"\"\"\n",
    "\n",
    "    new_edges= {'source_id':[],'target_id':[],'reward':[]}\n",
    "    for e in network['edges']:\n",
    "        new_edges['source_id'].append(e['source_id'])\n",
    "        new_edges['target_id'].append(e['target_id'])\n",
    "        new_edges['reward'].append(e['reward'])\n",
    "    return new_edges \n",
    "\n",
    "\n",
    "log_dir = r'C:\\Users\\Sara Bonati\\Desktop\\MPI_work\\Machines\\Reward_network_task\\logs\\dev'\n",
    "\n",
    "class Reward_Network(gym.Env):\n",
    "    \n",
    "    def __init__(self, network, to_log=False):\n",
    "        \n",
    "        #-------------\n",
    "        # assert tests\n",
    "        #-------------\n",
    "\n",
    "        # reward network information from json file (can be just one network or multiple networks)\n",
    "        self.network = network\n",
    "       \n",
    "        # initial reward and step values\n",
    "        self.INIT_REWARD = 0\n",
    "        self.INIT_STEP = 0\n",
    "        self.MAX_STEP = 8\n",
    "        self.N_NODES = 10\n",
    "\n",
    "        # network info\n",
    "        #self.id = self.network['network_id']\n",
    "        \n",
    "        self.nodes = torch.stack([torch.arange(10)]*3,dim = 0)\n",
    "\n",
    "        # initialize action space\n",
    "        self.buffer_action_space = torch.full((self.N_NODES, self.N_NODES), 3).long()\n",
    "        self.action_space = torch.full((len(self.network),self.N_NODES, self.N_NODES), 3).long()\n",
    "        self.new_edges = list(map(restructure_edges,network))\n",
    "        for n in range(len(self.network)):\n",
    "            source = torch.tensor(self.new_edges[n]['source_id']).long()\n",
    "            target = torch.tensor(self.new_edges[n]['target_id']).long()\n",
    "            reward = torch.tensor(self.new_edges[n]['reward']).long()\n",
    "            self.buffer_action_space[source,target]=reward\n",
    "            self.action_space[n,:,:] = self.buffer_action_space\n",
    "\n",
    "        \n",
    "        self.possible_rewards = [-100, -20, 0, 20, 140]\n",
    "        #self.reward_range = (min(self.possible_rewards)*self.MAX_STEP,self.network['max_reward'])\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.reward_balance = torch.full((len(self.network),1),self.INIT_REWARD)\n",
    "        self.step_counter = torch.full((len(self.network),1),self.INIT_STEP)\n",
    "        self.is_done = False #torch.full((len(self.network),1),False)\n",
    "        self.current_node = torch.IntTensor(list(map(lambda n: n['starting_node'], self.network)))\n",
    "\n",
    "        print('Environment initialized: \\n')\n",
    "        print(f'- set of nodes of shape {self.nodes.shape}')\n",
    "        print(f'- action space of shape {self.action_space.shape}')\n",
    "        print(f'- reward balance of shape {self.reward_balance.shape}')\n",
    "        print(f'- step counter of shape {self.step_counter.shape}')\n",
    "        print(f'- current node of shape {self.current_node.shape}')\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        self.source_node = self.current_node\n",
    "        print(f'Source nodes are: {self.current_node}')\n",
    "        self.rewards = torch.unsqueeze( torch.tensor([self.action_space[i,self.current_node[i],action[i]] for i in range(len(self.network))]), dim=-1)\n",
    "        print(f'We get rewards : {self.rewards[:,0]}')\n",
    "        self.reward_balance = torch.add(self.reward_balance,self.rewards)\n",
    "        print(f'New reward balance is: {self.reward_balance[:,0]}')\n",
    "        self.current_node = action\n",
    "        print(f'Now we are in nodes: {self.current_node}')\n",
    "        self.step_counter = torch.add(self.step_counter,1)\n",
    "        print(f'Step counter for all networks is: {self.step_counter[:,0]}')\n",
    "        print('\\n')\n",
    "\n",
    "        if torch.all(self.step_counter == 8):\n",
    "            self.is_done = True\n",
    "           \n",
    "        #return {'source_node':self.source_node,\n",
    "        #        'current_node':self.current_node,\n",
    "        #        'reward':action['reward'],\n",
    "        #        'total_reward':self.reward_balance,\n",
    "        #        'n_steps':self.step_counter,\n",
    "        #        'done':self.is_done}\n",
    "\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        TODO: CHANGE FOR VECTORIZATION\n",
    "        this function returns the current state of the environment.\n",
    "        State information given by this funciton is less detailed compared\n",
    "        to the observation. \n",
    "        \"\"\"\n",
    "        return {'current_node':self.current_node,\n",
    "                'total_reward':self.reward_balance,\n",
    "                'n_steps':self.step_counter,\n",
    "                'done':self.is_done}\n",
    "\n",
    "    def observe(self):\n",
    "        \"\"\"\n",
    "        TODO: CHANGE FOR VECTORIZATION\n",
    "        this function returns observation from the environment\n",
    "        \"\"\"\n",
    "        return {'current_node':self.current_node,\n",
    "                'actions_available':[n for n in self.action_space if n['source_id'] == self.current_node],\n",
    "                'next_possible_nodes':np.asarray([n['target_id'] for n in self.action_space if n['source_id'] == self.current_node]),\n",
    "                'next_possible_rewards':np.asarray([n['reward'] for n in self.action_space if n['source_id'] == self.current_node]),\n",
    "                'total_reward':self.reward_balance,\n",
    "                'n_steps':self.step_counter,\n",
    "                'done':self.is_done}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load a few networks and test (with random policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=r\"C:\\Users\\Sara Bonati\\Desktop\\MPI_work\\Machines\\Reward_network_task\\data\\rawdata\"\n",
    "with open(os.path.join(data_dir,'train.json')) as json_file:\n",
    "    train = json.load(json_file)\n",
    "\n",
    "test = train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized: \n",
      "\n",
      "- set of nodes of shape torch.Size([3, 10])\n",
      "- action space of shape torch.Size([3, 10, 10])\n",
      "- reward balance of shape torch.Size([3, 1])\n",
      "- step counter of shape torch.Size([3, 1])\n",
      "- current node of shape torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "N = Reward_Network(test)\n",
    "N.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current node in the envs: tensor([0, 0, 0], dtype=torch.int32)\n",
      "all_edges_source (all the row indices of actions where reward is not null)\n",
      "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,\n",
      "        7, 7, 7, 8, 8, 8, 9, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "        5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9])\n",
      "all_edges_target (all the column indices of actions where reward is not null)\n",
      "tensor([2, 5, 0, 4, 1, 3, 1, 4, 0, 2, 3, 6, 7, 8, 5, 9, 6, 9, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 5, 0, 1, 4, 5, 0, 2, 3, 0, 3, 6, 7, 8, 9, 5,\n",
      "        6, 8, 9, 6, 7, 9, 4, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 2, 3, 5, 7, 0, 3, 6,\n",
      "        8, 0, 7, 8, 9, 5, 6, 8, 9, 5, 6, 7, 9, 2, 3, 4, 7, 8])\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([0, 1]),)\n",
      "(tensor([0, 1, 2, 3]),)\n",
      "(tensor([0, 1, 2, 3]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([2, 5])\n",
      "tensor([1, 2, 3, 5])\n",
      "tensor([1, 2, 3, 5])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([5, 1, 3])\n",
      "Source nodes are: tensor([0, 0, 0], dtype=torch.int32)\n",
      "We get rewards : tensor([-100,   20,   20])\n",
      "New reward balance is: tensor([-100,   20,   20])\n",
      "Now we are in nodes: tensor([5, 1, 3])\n",
      "Step counter for all networks is: tensor([1, 1, 1])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([5, 1, 3])\n",
      "all_edges_source (all the row indices of actions where reward is not null)\n",
      "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,\n",
      "        7, 7, 7, 8, 8, 8, 9, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "        5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9])\n",
      "all_edges_target (all the column indices of actions where reward is not null)\n",
      "tensor([2, 5, 0, 4, 1, 3, 1, 4, 0, 2, 3, 6, 7, 8, 5, 9, 6, 9, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 5, 0, 1, 4, 5, 0, 2, 3, 0, 3, 6, 7, 8, 9, 5,\n",
      "        6, 8, 9, 6, 7, 9, 4, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 2, 3, 5, 7, 0, 3, 6,\n",
      "        8, 0, 7, 8, 9, 5, 6, 8, 9, 5, 6, 7, 9, 2, 3, 4, 7, 8])\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([10, 11]),)\n",
      "(tensor([4, 5, 6]),)\n",
      "(tensor([11, 12, 13, 14, 15]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([3, 6])\n",
      "tensor([0, 2, 4])\n",
      "tensor([0, 1, 2, 4, 5])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([3, 4, 2])\n",
      "Source nodes are: tensor([5, 1, 3])\n",
      "We get rewards : tensor([ 0,  0, 20])\n",
      "New reward balance is: tensor([-100,   20,   40])\n",
      "Now we are in nodes: tensor([3, 4, 2])\n",
      "Step counter for all networks is: tensor([2, 2, 2])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([3, 4, 2])\n",
      "all_edges_source (all the row indices of actions where reward is not null)\n",
      "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,\n",
      "        7, 7, 7, 8, 8, 8, 9, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "        5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9])\n",
      "all_edges_target (all the column indices of actions where reward is not null)\n",
      "tensor([2, 5, 0, 4, 1, 3, 1, 4, 0, 2, 3, 6, 7, 8, 5, 9, 6, 9, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 5, 0, 1, 4, 5, 0, 2, 3, 0, 3, 6, 7, 8, 9, 5,\n",
      "        6, 8, 9, 6, 7, 9, 4, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 2, 3, 5, 7, 0, 3, 6,\n",
      "        8, 0, 7, 8, 9, 5, 6, 8, 9, 5, 6, 7, 9, 2, 3, 4, 7, 8])\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([6, 7]),)\n",
      "(tensor([14, 15, 16]),)\n",
      "(tensor([ 7,  8,  9, 10]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([1, 4])\n",
      "tensor([0, 2, 3])\n",
      "tensor([1, 3, 4, 5])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([1, 0, 1])\n",
      "Source nodes are: tensor([3, 4, 2])\n",
      "We get rewards : tensor([ 20, -20,  20])\n",
      "New reward balance is: tensor([-80,   0,  60])\n",
      "Now we are in nodes: tensor([1, 0, 1])\n",
      "Step counter for all networks is: tensor([3, 3, 3])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([1, 0, 1])\n",
      "all_edges_source (all the row indices of actions where reward is not null)\n",
      "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,\n",
      "        7, 7, 7, 8, 8, 8, 9, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "        5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9])\n",
      "all_edges_target (all the column indices of actions where reward is not null)\n",
      "tensor([2, 5, 0, 4, 1, 3, 1, 4, 0, 2, 3, 6, 7, 8, 5, 9, 6, 9, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 5, 0, 1, 4, 5, 0, 2, 3, 0, 3, 6, 7, 8, 9, 5,\n",
      "        6, 8, 9, 6, 7, 9, 4, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 2, 3, 5, 7, 0, 3, 6,\n",
      "        8, 0, 7, 8, 9, 5, 6, 8, 9, 5, 6, 7, 9, 2, 3, 4, 7, 8])\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([2, 3]),)\n",
      "(tensor([0, 1, 2, 3]),)\n",
      "(tensor([4, 5, 6]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([0, 4])\n",
      "tensor([1, 2, 3, 5])\n",
      "tensor([0, 2, 4])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([4, 5, 0])\n",
      "Source nodes are: tensor([1, 0, 1])\n",
      "We get rewards : tensor([  20, -100,    0])\n",
      "New reward balance is: tensor([ -60, -100,   60])\n",
      "Now we are in nodes: tensor([4, 5, 0])\n",
      "Step counter for all networks is: tensor([4, 4, 4])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([4, 5, 0])\n",
      "all_edges_source (all the row indices of actions where reward is not null)\n",
      "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,\n",
      "        7, 7, 7, 8, 8, 8, 9, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "        5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9])\n",
      "all_edges_target (all the column indices of actions where reward is not null)\n",
      "tensor([2, 5, 0, 4, 1, 3, 1, 4, 0, 2, 3, 6, 7, 8, 5, 9, 6, 9, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 5, 0, 1, 4, 5, 0, 2, 3, 0, 3, 6, 7, 8, 9, 5,\n",
      "        6, 8, 9, 6, 7, 9, 4, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 2, 3, 5, 7, 0, 3, 6,\n",
      "        8, 0, 7, 8, 9, 5, 6, 8, 9, 5, 6, 7, 9, 2, 3, 4, 7, 8])\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([8, 9]),)\n",
      "(tensor([17, 18, 19]),)\n",
      "(tensor([0, 1, 2, 3]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([0, 2])\n",
      "tensor([0, 3, 6])\n",
      "tensor([1, 2, 3, 5])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([2, 0, 1])\n",
      "Source nodes are: tensor([4, 5, 0])\n",
      "We get rewards : tensor([ 20, -20,  20])\n",
      "New reward balance is: tensor([ -40, -120,   80])\n",
      "Now we are in nodes: tensor([2, 0, 1])\n",
      "Step counter for all networks is: tensor([5, 5, 5])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([2, 0, 1])\n",
      "all_edges_source (all the row indices of actions where reward is not null)\n",
      "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,\n",
      "        7, 7, 7, 8, 8, 8, 9, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "        5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9])\n",
      "all_edges_target (all the column indices of actions where reward is not null)\n",
      "tensor([2, 5, 0, 4, 1, 3, 1, 4, 0, 2, 3, 6, 7, 8, 5, 9, 6, 9, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 5, 0, 1, 4, 5, 0, 2, 3, 0, 3, 6, 7, 8, 9, 5,\n",
      "        6, 8, 9, 6, 7, 9, 4, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 2, 3, 5, 7, 0, 3, 6,\n",
      "        8, 0, 7, 8, 9, 5, 6, 8, 9, 5, 6, 7, 9, 2, 3, 4, 7, 8])\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([4, 5]),)\n",
      "(tensor([0, 1, 2, 3]),)\n",
      "(tensor([4, 5, 6]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([1, 3])\n",
      "tensor([1, 2, 3, 5])\n",
      "tensor([0, 2, 4])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([3, 2, 4])\n",
      "Source nodes are: tensor([2, 0, 1])\n",
      "We get rewards : tensor([ -20,  -20, -100])\n",
      "New reward balance is: tensor([ -60, -140,  -20])\n",
      "Now we are in nodes: tensor([3, 2, 4])\n",
      "Step counter for all networks is: tensor([6, 6, 6])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([3, 2, 4])\n",
      "all_edges_source (all the row indices of actions where reward is not null)\n",
      "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,\n",
      "        7, 7, 7, 8, 8, 8, 9, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "        5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9])\n",
      "all_edges_target (all the column indices of actions where reward is not null)\n",
      "tensor([2, 5, 0, 4, 1, 3, 1, 4, 0, 2, 3, 6, 7, 8, 5, 9, 6, 9, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 5, 0, 1, 4, 5, 0, 2, 3, 0, 3, 6, 7, 8, 9, 5,\n",
      "        6, 8, 9, 6, 7, 9, 4, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 2, 3, 5, 7, 0, 3, 6,\n",
      "        8, 0, 7, 8, 9, 5, 6, 8, 9, 5, 6, 7, 9, 2, 3, 4, 7, 8])\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([6, 7]),)\n",
      "(tensor([7, 8, 9]),)\n",
      "(tensor([16, 17, 18, 19, 20]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([1, 4])\n",
      "tensor([1, 3, 5])\n",
      "tensor([0, 2, 3, 5, 7])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([4, 5, 5])\n",
      "Source nodes are: tensor([3, 2, 4])\n",
      "We get rewards : tensor([ -20, -100,   20])\n",
      "New reward balance is: tensor([ -80, -240,    0])\n",
      "Now we are in nodes: tensor([4, 5, 5])\n",
      "Step counter for all networks is: tensor([7, 7, 7])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([4, 5, 5])\n",
      "all_edges_source (all the row indices of actions where reward is not null)\n",
      "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,\n",
      "        7, 7, 7, 8, 8, 8, 9, 9, 9])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "        5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9])\n",
      "all_edges_target (all the column indices of actions where reward is not null)\n",
      "tensor([2, 5, 0, 4, 1, 3, 1, 4, 0, 2, 3, 6, 7, 8, 5, 9, 6, 9, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 5, 0, 1, 4, 5, 0, 2, 3, 0, 3, 6, 7, 8, 9, 5,\n",
      "        6, 8, 9, 6, 7, 9, 4, 7, 8])\n",
      "tensor([1, 2, 3, 5, 0, 2, 4, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 2, 3, 5, 7, 0, 3, 6,\n",
      "        8, 0, 7, 8, 9, 5, 6, 8, 9, 5, 6, 7, 9, 2, 3, 4, 7, 8])\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([8, 9]),)\n",
      "(tensor([17, 18, 19]),)\n",
      "(tensor([21, 22, 23, 24]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([0, 2])\n",
      "tensor([0, 3, 6])\n",
      "tensor([0, 3, 6, 8])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([0, 3, 0])\n",
      "Source nodes are: tensor([4, 5, 5])\n",
      "We get rewards : tensor([-20,   0, -20])\n",
      "New reward balance is: tensor([-100, -240,  -20])\n",
      "Now we are in nodes: tensor([0, 3, 0])\n",
      "Step counter for all networks is: tensor([8, 8, 8])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while N.is_done==False:\n",
    "    print(f'Current node in the envs: {N.current_node}')\n",
    "    \n",
    "    all_edges_source = [torch.where(N.action_space[i,:,:]!=3)[0] for i in range(len(test))]\n",
    "    print(f'all_edges_source (all the row indices of actions where reward is not null)')\n",
    "    print(*all_edges_source,sep='\\n')\n",
    "    all_edges_target = [torch.where(N.action_space[i,:,:]!=3)[1] for i in range(len(test))]\n",
    "    print(f'all_edges_target (all the column indices of actions where reward is not null)')\n",
    "    print(*all_edges_target,sep='\\n')\n",
    "    valid_edges_source = [torch.where(all_edges_source[i]==N.current_node[i]) for i in range(len(test))]\n",
    "    print(f'valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)')\n",
    "    print(*valid_edges_source,sep='\\n')\n",
    "    valid_edges_target = [all_edges_target[i][valid_edges_source[i]] for i in range(len(test))]\n",
    "    print(f'valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)')\n",
    "    print(*valid_edges_target,sep='\\n')\n",
    "    print('\\n')\n",
    "    random_action = torch.stack(list(map(lambda x: random.choice(x),valid_edges_target)), dim=0)\n",
    "    print(f'Actions (or target edge) ranodmly selected in each env: {random_action}')\n",
    "\n",
    "    N.step(random_action)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f89c1238541db4369507a7223b8d243e8becdb3ef9d6ae1daa1e85f92c02f552"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
