{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy of environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from collections import Counter\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "def restructure_edges(network):\n",
    "    \"\"\"\n",
    "    This function restructures the edges from list of dicts\n",
    "    to one dict, to improve construction of edges matrix and \n",
    "    env vectorization\n",
    "    \"\"\"\n",
    "\n",
    "    new_edges= {'source_id':[],'target_id':[],'reward':[]}\n",
    "    for e in network['edges']:\n",
    "        new_edges['source_id'].append(e['source_id'])\n",
    "        new_edges['target_id'].append(e['target_id'])\n",
    "        new_edges['reward'].append(e['reward'])\n",
    "    return new_edges \n",
    "\n",
    "\n",
    "class Reward_Network(gym.Env):\n",
    "    \n",
    "    def __init__(self, network, to_log=False):\n",
    "        \n",
    "        #-------------\n",
    "        # assert tests TODO\n",
    "        #-------------\n",
    "\n",
    "        # reward network information from json file (can be just one network or multiple networks)\n",
    "        self.network = network\n",
    "       \n",
    "        # initial reward and step values\n",
    "        self.INIT_REWARD = 0\n",
    "        self.INIT_STEP = 0\n",
    "        self.MAX_STEP = 8\n",
    "        self.N_NODES = 10\n",
    "        self.N_NETWORKS = len(self.network)\n",
    "\n",
    "        # define node numbers (from 0 to 9)\n",
    "        self.nodes = torch.stack([torch.arange(10)]*self.N_NETWORKS,dim = 0)\n",
    "\n",
    "        # initialize action space (\"reward adjacency matrix\")\n",
    "        # NOTE intially I thought about the value 0 to be the value that signals that there is no edge between two nodes,\n",
    "        # however since 0 is also a possible reward I have put 1 as the value in the reward adjacency matrix that represents\n",
    "        # no edge between two nodes\n",
    "        self.buffer_action_space = torch.full((self.N_NODES, self.N_NODES), 1).long()  \n",
    "        self.action_space = torch.full((self.N_NETWORKS,self.N_NODES, self.N_NODES), 1).long()  \n",
    "        self.new_edges = list(map(restructure_edges,network))\n",
    "        for n in range(self.N_NETWORKS):\n",
    "            source = torch.tensor(self.new_edges[n]['source_id']).long()\n",
    "            target = torch.tensor(self.new_edges[n]['target_id']).long()\n",
    "            reward = torch.tensor(self.new_edges[n]['reward']).long()\n",
    "            self.buffer_action_space[source,target]=reward\n",
    "            self.action_space[n,:,:] = self.buffer_action_space\n",
    "\n",
    "\n",
    "        print(f'example of action space for 1 env: {self.action_space[0,:,:]}')\n",
    "        # all_edges_source (all the row indices of actions where reward is not null)\n",
    "        self.all_edges_source = [torch.where(self.action_space[i,:,:]!=1)[0] for i in range(self.N_NETWORKS)] \n",
    "        print(f'example of all edges source for 1 env: {self.all_edges_source[0]}')\n",
    "        # all_edges_target (all the column indices of actions where reward is not null)\n",
    "        self.all_edges_target = [torch.where(self.action_space[i,:,:]!=1)[1] for i in range(self.N_NETWORKS)] \n",
    "        print(f'example of all edges target for 1 env: {self.all_edges_target[0]}')\n",
    "        \n",
    "        self.possible_rewards = [-100, -20, 0, 20, 140]\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.reward_balance = torch.full((self.N_NETWORKS,1),self.INIT_REWARD)\n",
    "        self.step_counter = torch.full((self.N_NETWORKS,1),self.INIT_STEP)\n",
    "        self.is_done = False #torch.full((len(self.network),1),False)\n",
    "        self.current_node = torch.IntTensor(list(map(lambda n: n['starting_node'], self.network)))\n",
    "\n",
    "        print('Environment initialized: \\n')\n",
    "        print(f'- set of nodes of shape {self.nodes.shape}')\n",
    "        print(f'- action space of shape {self.action_space.shape}')\n",
    "        print(f'- reward balance of shape {self.reward_balance.shape}')\n",
    "        print(f'- step counter of shape {self.step_counter.shape}')\n",
    "        print(f'- current node of shape {self.current_node.shape}')\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Take a step in all environments; here action corresponds to the target nodes for each env\n",
    "        action_i \\in [0,1,2,3,4,5,6,7,8,9]\n",
    "        '''\n",
    "\n",
    "        self.source_node = self.current_node\n",
    "        print(f'Source nodes are: {self.current_node}, we are going to new nodes {action}')\n",
    "        self.rewards = torch.unsqueeze( torch.tensor([self.action_space[i,self.current_node[i],action[i]] for i in range(self.N_NETWORKS)]), dim=-1)\n",
    "        print(f'We get rewards : {self.rewards[:,0]}')\n",
    "        self.reward_balance = torch.add(self.reward_balance,self.rewards)\n",
    "        print(f'New reward balance is: {self.reward_balance[:,0]}')\n",
    "        self.current_node = action\n",
    "        print(f'Now we are in nodes: {self.current_node}')\n",
    "        self.step_counter = torch.add(self.step_counter,1)\n",
    "        print(f'Step counter for all networks is: {self.step_counter[:,0]}')\n",
    "        print('\\n')\n",
    "\n",
    "        if torch.all(self.step_counter == 8):\n",
    "            self.is_done = True\n",
    "           \n",
    "        #return {'source_node':self.source_node,\n",
    "        #        'current_node':self.current_node,\n",
    "        #        'reward':action['reward'],\n",
    "        #        'total_reward':self.reward_balance,\n",
    "        #        'n_steps':self.step_counter,\n",
    "        #        'done':self.is_done}\n",
    "\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        this function returns the current state of the environment.\n",
    "        State information given by this funciton is less detailed compared\n",
    "        to the observation. \n",
    "        \"\"\"\n",
    "        return {'current_node':self.current_node,\n",
    "                'total_reward':self.reward_balance,\n",
    "                'n_steps':self.step_counter,\n",
    "                'done':self.is_done}\n",
    "\n",
    "\n",
    "    def get_possible_rewards(self,env_action_space,source_idx,target_idx):\n",
    "        \"\"\"\n",
    "        this function returns the rewards associated to the row (source node)\n",
    "        and column (target node) indices of an environment\n",
    "\n",
    "        TODO: fix\n",
    "        \"\"\"\n",
    "        return torch.gather(env_action_space,1,target_idx)\n",
    "        #return env_action_space[source_idx.chunk(chunks=len(source_idx),dim=0),target_idx.chunk(chunks=len(target_idx),dim=0)]\n",
    "\n",
    "    def observe(self):\n",
    "        \"\"\"\n",
    "        TODO: CHANGE FOR VECTORIZATION\n",
    "        this function returns observation from the environment\n",
    "        \"\"\"\n",
    "        #self.valid_edges_source = [torch.where(self.all_edges_source[i]==self.current_node[i])[:] for i in range(self.N_NETWORKS)]\n",
    "        self.valid_edges_source = [torch.where(self.all_edges_source[i]==self.current_node[i])[0][:] for i in range(self.N_NETWORKS)]\n",
    "        self.valid_edges_target = [self.all_edges_target[i][self.valid_edges_source[i]] for i in range(self.N_NETWORKS)]\n",
    "\n",
    "        return {'current_node':self.current_node,\n",
    "                'next_possible_nodes':self.valid_edges_target,\n",
    "                #'next_possible_rewards':torch.stack(list(map(lambda x: x[self.valid_edges_source,self.valid_edges_target],self.action_space)), dim=0),\n",
    "                'next_possible_rewards':[self.get_possible_rewards(self.action_space[i,:,:],self.valid_edges_source[i],self.valid_edges_target[i]) for i in range(self.N_NETWORKS)],\n",
    "                'total_reward':self.reward_balance,\n",
    "                'n_steps':self.step_counter,\n",
    "                'done':self.is_done}\n",
    "\n",
    "    #def observe(self):\n",
    "    #    \"\"\"\n",
    "    #    TODO: CHANGE FOR VECTORIZATION\n",
    "    #    this function returns observation from the environment\n",
    "    #    \"\"\"\n",
    "    #    return {'current_node':self.current_node,\n",
    "    #            'actions_available':[n for n in self.action_space if n['source_id'] == self.current_node],\n",
    "    #            'next_possible_nodes':np.asarray([n['target_id'] for n in self.action_space if n['source_id'] == self.current_node]),\n",
    "    #            'next_possible_rewards':np.asarray([n['reward'] for n in self.action_space if n['source_id'] == self.current_node]),\n",
    "    #            'total_reward':self.reward_balance,\n",
    "    #            'n_steps':self.step_counter,\n",
    "    #            'done':self.is_done}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load a few networks and test (with random policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=r\"/Users/bonati/Desktop/CHM/reward_networks/data/rawdata\"\n",
    "with open(os.path.join(data_dir,'train.json')) as json_file:\n",
    "    train = json.load(json_file)\n",
    "\n",
    "test = train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of action space for 1 env: tensor([[   1,    1,  -20,    1,    1, -100,    1,    1,    1,    1],\n",
      "        [  20,    1,    1,    1,   20,    1,    1,    1,    1,    1],\n",
      "        [   1,    0,    1,  -20,    1,    1,    1,    1,    1,    1],\n",
      "        [   1,   20,    1,    1,  -20,    1,    1,    1,    1,    1],\n",
      "        [ -20,    1,   20,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   1,    1,    1,    0,    1,    1,    0,    1,    1,    1],\n",
      "        [   1,    1,    1,    1,    1,    1,    1,    0,   20,    1],\n",
      "        [   1,    1,    1,    1,    1,  -20,    1,    1,    1,    0],\n",
      "        [   1,    1,    1,    1,    1,    1,   20,    1,    1,   20],\n",
      "        [   1,    1,    1,    1,    1,    1,    1,  140,  140,    1]])\n",
      "example of all edges source for 1 env: tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "example of all edges target for 1 env: tensor([2, 5, 0, 4, 1, 3, 1, 4, 0, 2, 3, 6, 7, 8, 5, 9, 6, 9, 7, 8])\n",
      "Environment initialized: \n",
      "\n",
      "- set of nodes of shape torch.Size([5, 10])\n",
      "- action space of shape torch.Size([5, 10, 10])\n",
      "- reward balance of shape torch.Size([5, 1])\n",
      "- step counter of shape torch.Size([5, 1])\n",
      "- current node of shape torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "N = Reward_Network(test)\n",
    "N.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    1,  -20,    1,    1, -100,    1,    1,    1,    1],\n",
       "        [  20,    1,    1,    1,   20,    1,    1,    1,    1,    1],\n",
       "        [   1,    0,    1,  -20,    1,    1,    1,    1,    1,    1],\n",
       "        [   1,   20,    1,    1,  -20,    1,    1,    1,    1,    1],\n",
       "        [ -20,    1,   20,    1,    1,    1,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    0,    1,    1,    0,    1,    1,    1],\n",
       "        [   1,    1,    1,    1,    1,    1,    1,    0,   20,    1],\n",
       "        [   1,    1,    1,    1,    1,  -20,    1,    1,    1,    0],\n",
       "        [   1,    1,    1,    1,    1,    1,   20,    1,    1,   20],\n",
       "        [   1,    1,    1,    1,    1,    1,    1,  140,  140,    1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.action_space[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m N\u001b[39m.\u001b[39;49mobserve()\n",
      "\u001b[1;32m/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb Cell 7\u001b[0m in \u001b[0;36mReward_Network.observe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_edges_source \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mwhere(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_edges_source[i]\u001b[39m==\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_node[i])[\u001b[39m0\u001b[39m][:] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN_NETWORKS)]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_edges_target \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_edges_target[i][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_edges_source[i]] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN_NETWORKS)]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mcurrent_node\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_node,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnext_possible_nodes\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_edges_target,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m         \u001b[39m#'next_possible_rewards':torch.stack(list(map(lambda x: x[self.valid_edges_source,self.valid_edges_target],self.action_space)), dim=0),\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnext_possible_rewards\u001b[39m\u001b[39m'\u001b[39m:[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_possible_rewards(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space[i,:,:],\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_edges_source[i],\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_edges_target[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN_NETWORKS)],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtotal_reward\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_balance,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mn_steps\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_done}\n",
      "\u001b[1;32m/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb Cell 7\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_edges_source \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mwhere(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_edges_source[i]\u001b[39m==\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_node[i])[\u001b[39m0\u001b[39m][:] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN_NETWORKS)]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_edges_target \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_edges_target[i][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_edges_source[i]] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN_NETWORKS)]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mcurrent_node\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_node,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnext_possible_nodes\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_edges_target,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m         \u001b[39m#'next_possible_rewards':torch.stack(list(map(lambda x: x[self.valid_edges_source,self.valid_edges_target],self.action_space)), dim=0),\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnext_possible_rewards\u001b[39m\u001b[39m'\u001b[39m:[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_possible_rewards(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_space[i,:,:],\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalid_edges_source[i],\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalid_edges_target[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN_NETWORKS)],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtotal_reward\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_balance,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mn_steps\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_done}\n",
      "\u001b[1;32m/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb Cell 7\u001b[0m in \u001b[0;36mReward_Network.get_possible_rewards\u001b[0;34m(self, env_action_space, source_idx, target_idx)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_possible_rewards\u001b[39m(\u001b[39mself\u001b[39m,env_action_space,source_idx,target_idx):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39m    this function returns the rewards associated to the row (source node)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m \u001b[39m    and column (target node) indices of an environment\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39m    TODO: fix\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/bonati/Desktop/CHM/reward_networks/reward-network-iii-algorithm/notebooks/try_vectorization.ipynb#W6sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgather(env_action_space,\u001b[39m1\u001b[39;49m,target_idx)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "N.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current node in the envs: tensor([0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "---------------------------------------------\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([0, 1]),)\n",
      "(tensor([0, 1, 2, 3]),)\n",
      "(tensor([0, 1, 2, 3]),)\n",
      "(tensor([0, 1, 2, 3, 4, 5]),)\n",
      "(tensor([0, 1, 2, 3, 4, 5]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([2, 5])\n",
      "tensor([1, 2, 3, 5])\n",
      "tensor([1, 2, 3, 5])\n",
      "tensor([1, 2, 3, 5, 6, 7])\n",
      "tensor([1, 2, 3, 5, 6, 7])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([5, 1, 1, 7, 7])\n",
      "Source nodes are: tensor([0, 0, 0, 0, 0], dtype=torch.int32), we are going to new nodes tensor([5, 1, 1, 7, 7])\n",
      "We get rewards : tensor([-100,   20,   20, -100, -100])\n",
      "New reward balance is: tensor([-100,   20,   20, -100, -100])\n",
      "Now we are in nodes: tensor([5, 1, 1, 7, 7])\n",
      "Step counter for all networks is: tensor([1, 1, 1, 1, 1])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([5, 1, 1, 7, 7])\n",
      "---------------------------------------------\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([10, 11]),)\n",
      "(tensor([4, 5, 6]),)\n",
      "(tensor([4, 5, 6]),)\n",
      "(tensor([35, 36, 37, 38]),)\n",
      "(tensor([36, 37, 38, 39, 40]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([3, 6])\n",
      "tensor([0, 2, 4])\n",
      "tensor([0, 2, 4])\n",
      "tensor([5, 6, 8, 9])\n",
      "tensor([4, 5, 6, 8, 9])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([6, 4, 2, 8, 9])\n",
      "Source nodes are: tensor([5, 1, 1, 7, 7]), we are going to new nodes tensor([6, 4, 2, 8, 9])\n",
      "We get rewards : tensor([ 0,  0, 20, 20,  0])\n",
      "New reward balance is: tensor([-100,   20,   40,  -80, -100])\n",
      "Now we are in nodes: tensor([6, 4, 2, 8, 9])\n",
      "Step counter for all networks is: tensor([2, 2, 2, 2, 2])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([6, 4, 2, 8, 9])\n",
      "---------------------------------------------\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([12, 13]),)\n",
      "(tensor([14, 15, 16]),)\n",
      "(tensor([ 7,  8,  9, 10]),)\n",
      "(tensor([39, 40, 41, 42, 43]),)\n",
      "(tensor([47, 48, 49, 50, 51]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([7, 8])\n",
      "tensor([0, 2, 3])\n",
      "tensor([1, 3, 4, 5])\n",
      "tensor([0, 5, 6, 7, 9])\n",
      "tensor([2, 3, 4, 7, 8])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([7, 0, 5, 9, 8])\n",
      "Source nodes are: tensor([6, 4, 2, 8, 9]), we are going to new nodes tensor([7, 0, 5, 9, 8])\n",
      "We get rewards : tensor([   0,  -20, -100,   20,  140])\n",
      "New reward balance is: tensor([-100,    0,  -60,  -60,   40])\n",
      "Now we are in nodes: tensor([7, 0, 5, 9, 8])\n",
      "Step counter for all networks is: tensor([3, 3, 3, 3, 3])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([7, 0, 5, 9, 8])\n",
      "---------------------------------------------\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([14, 15]),)\n",
      "(tensor([0, 1, 2, 3]),)\n",
      "(tensor([21, 22, 23, 24]),)\n",
      "(tensor([44, 45, 46, 47, 48]),)\n",
      "(tensor([41, 42, 43, 44, 45, 46]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([5, 9])\n",
      "tensor([1, 2, 3, 5])\n",
      "tensor([0, 3, 6, 8])\n",
      "tensor([2, 3, 4, 7, 8])\n",
      "tensor([0, 1, 5, 6, 7, 9])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([5, 3, 3, 2, 1])\n",
      "Source nodes are: tensor([7, 0, 5, 9, 8]), we are going to new nodes tensor([5, 3, 3, 2, 1])\n",
      "We get rewards : tensor([-20,  20,   0, 140, -20])\n",
      "New reward balance is: tensor([-120,   20,  -60,   80,   20])\n",
      "Now we are in nodes: tensor([5, 3, 3, 2, 1])\n",
      "Step counter for all networks is: tensor([4, 4, 4, 4, 4])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([5, 3, 3, 2, 1])\n",
      "---------------------------------------------\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([10, 11]),)\n",
      "(tensor([10, 11, 12, 13]),)\n",
      "(tensor([11, 12, 13, 14, 15]),)\n",
      "(tensor([10, 11, 12, 13, 14]),)\n",
      "(tensor([ 6,  7,  8,  9, 10]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([3, 6])\n",
      "tensor([0, 1, 4, 5])\n",
      "tensor([0, 1, 2, 4, 5])\n",
      "tensor([1, 3, 4, 5, 7])\n",
      "tensor([0, 2, 3, 4, 5])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([3, 1, 0, 4, 3])\n",
      "Source nodes are: tensor([5, 3, 3, 2, 1]), we are going to new nodes tensor([3, 1, 0, 4, 3])\n",
      "We get rewards : tensor([   0,   20,    0, -100,    0])\n",
      "New reward balance is: tensor([-120,   40,  -60,  -20,   20])\n",
      "Now we are in nodes: tensor([3, 1, 0, 4, 3])\n",
      "Step counter for all networks is: tensor([5, 5, 5, 5, 5])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([3, 1, 0, 4, 3])\n",
      "---------------------------------------------\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([6, 7]),)\n",
      "(tensor([4, 5, 6]),)\n",
      "(tensor([0, 1, 2, 3]),)\n",
      "(tensor([20, 21, 22, 23, 24, 25]),)\n",
      "(tensor([16, 17, 18, 19, 20]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([1, 4])\n",
      "tensor([0, 2, 4])\n",
      "tensor([1, 2, 3, 5])\n",
      "tensor([0, 2, 3, 5, 7, 8])\n",
      "tensor([0, 1, 2, 4, 5])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([1, 0, 3, 2, 2])\n",
      "Source nodes are: tensor([3, 1, 0, 4, 3]), we are going to new nodes tensor([1, 0, 3, 2, 2])\n",
      "We get rewards : tensor([ 20,  20,  20,  20, -20])\n",
      "New reward balance is: tensor([-100,   60,  -40,    0,    0])\n",
      "Now we are in nodes: tensor([1, 0, 3, 2, 2])\n",
      "Step counter for all networks is: tensor([6, 6, 6, 6, 6])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([1, 0, 3, 2, 2])\n",
      "---------------------------------------------\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([2, 3]),)\n",
      "(tensor([0, 1, 2, 3]),)\n",
      "(tensor([11, 12, 13, 14, 15]),)\n",
      "(tensor([10, 11, 12, 13, 14]),)\n",
      "(tensor([11, 12, 13, 14, 15]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([0, 4])\n",
      "tensor([1, 2, 3, 5])\n",
      "tensor([0, 1, 2, 4, 5])\n",
      "tensor([1, 3, 4, 5, 7])\n",
      "tensor([1, 3, 4, 5, 7])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([0, 2, 5, 7, 5])\n",
      "Source nodes are: tensor([1, 0, 3, 2, 2]), we are going to new nodes tensor([0, 2, 5, 7, 5])\n",
      "We get rewards : tensor([  20,  -20, -100, -100, -100])\n",
      "New reward balance is: tensor([ -80,   40, -140, -100, -100])\n",
      "Now we are in nodes: tensor([0, 2, 5, 7, 5])\n",
      "Step counter for all networks is: tensor([7, 7, 7, 7, 7])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([0, 2, 5, 7, 5])\n",
      "---------------------------------------------\n",
      "valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)\n",
      "(tensor([0, 1]),)\n",
      "(tensor([7, 8, 9]),)\n",
      "(tensor([21, 22, 23, 24]),)\n",
      "(tensor([35, 36, 37, 38]),)\n",
      "(tensor([27, 28, 29, 30]),)\n",
      "valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)\n",
      "tensor([2, 5])\n",
      "tensor([1, 3, 5])\n",
      "tensor([0, 3, 6, 8])\n",
      "tensor([5, 6, 8, 9])\n",
      "tensor([0, 3, 6, 8])\n",
      "\n",
      "\n",
      "Actions (or target edge) ranodmly selected in each env: tensor([2, 5, 3, 5, 0])\n",
      "Source nodes are: tensor([0, 2, 5, 7, 5]), we are going to new nodes tensor([2, 5, 3, 5, 0])\n",
      "We get rewards : tensor([ -20, -100,    0,   20,  -20])\n",
      "New reward balance is: tensor([-100,  -60, -140,  -80, -120])\n",
      "Now we are in nodes: tensor([2, 5, 3, 5, 0])\n",
      "Step counter for all networks is: tensor([8, 8, 8, 8, 8])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while N.is_done==False:\n",
    "    print(f'Current node in the envs: {N.current_node}')\n",
    "    print('---------------------------------------------')\n",
    "    \n",
    "    valid_edges_source = [torch.where(N.all_edges_source[i]==N.current_node[i]) for i in range(len(test))]\n",
    "    print(f'valid_edges_source (all the row indices of actions where reward is not null AND the source node is the current node in env)')\n",
    "    print(*valid_edges_source,sep='\\n')\n",
    "    valid_edges_target = [N.all_edges_target[i][valid_edges_source[i]] for i in range(len(test))]\n",
    "    print(f'valid_edges_target (all the column indices of actions where reward is not null AND the source node is the current node in env)')\n",
    "    print(*valid_edges_target,sep='\\n')\n",
    "    print('\\n')\n",
    "    random_action = torch.stack(list(map(lambda x: random.choice(x),valid_edges_target)), dim=0)\n",
    "    print(f'Actions (or target edge) ranodmly selected in each env: {random_action}')\n",
    "\n",
    "    N.step(random_action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Gym.Spaces to define observation space of the environment in order to use stable-baselines3?\n",
    "\n",
    "https://stackoverflow.com/questions/56448260/how-could-i-define-the-observation-space-for-my-custom-openai-enviroment\n",
    "https://github.com/openai/gym/blob/master/gym/spaces/graph.py\n",
    "https://github.com/openai/gym/issues/2912\n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html (current version of env does not completely follow Gym API since we custom define the action and observation space without using the gym.spaces submodule)\n",
    "https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html\n",
    "\n",
    "\n",
    "The problem with the openai gym enviroments is that their space wrappers generate the action space and observation space but there doesn't seem to be much room for customization\n",
    "\n",
    "A possibility could be to define `Tuple(Discrete(10),Discrete(10))` or `MultiDiscrete ([10,10])` for the action space, and a utility function to check if actions are valid in the current node you're at.\n",
    "\n",
    "Observation space might be a bit trickier, should include at least current node, possible actions, step counter and done flag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7535b6583ac7e43eef2c602d4cb77470ba1eeca741c11a0535b54e0aa8791c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
