{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy of environment class\n",
    "\n",
    "TODO: reproduce work from agent based here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "def restructure_edges(network):\n",
    "    \"\"\"\n",
    "    This function restructures the edges from list of dicts\n",
    "    to one dict, to improve construction of edges matrix and \n",
    "    env vectorization\n",
    "    \"\"\"\n",
    "\n",
    "    new_edges= {'source_id':[],'target_id':[],'reward':[]}\n",
    "    for e in network['edges']:\n",
    "        new_edges['source_id'].append(e['source_id'])\n",
    "        new_edges['target_id'].append(e['target_id'])\n",
    "        new_edges['reward'].append(e['reward'])\n",
    "    return new_edges \n",
    "\n",
    "\n",
    "class Reward_Network(gym.Env):\n",
    "    \n",
    "    def __init__(self, network, to_log=False):\n",
    "        \n",
    "        #-------------\n",
    "        # assert tests TODO\n",
    "        #-------------\n",
    "\n",
    "        # reward network information from json file (can be just one network or multiple networks)\n",
    "        self.network = network\n",
    "       \n",
    "        # initial reward and step values\n",
    "        self.INIT_REWARD = 0\n",
    "        self.INIT_STEP = 0\n",
    "        self.MAX_STEP = 8\n",
    "        self.N_NODES = 10\n",
    "        self.N_NETWORKS = len(self.network)\n",
    "\n",
    "        # define node numbers (from 0 to 9)\n",
    "        self.nodes = torch.stack([torch.arange(10)]*self.N_NETWORKS,dim = 0)\n",
    "        # define starting nodes\n",
    "        self.starting_nodes = torch.tensor(list(map(lambda n: n['starting_node'], self.network)), dtype=torch.long)\n",
    "        # define possible rewards along with corresponding reward index\n",
    "        self.possible_rewards = {-100:1, -20:2, 0:3, 20:4, 140:5}\n",
    "\n",
    "        # initialize action space (\"reward adjacency matrix\")\n",
    "        # NOTE intially I thought about the value 0 to be the value that signals that there is no edge between two nodes,\n",
    "        # however since 0 is also a possible reward I have put 1 as the value in the reward adjacency matrix that represents\n",
    "        # no edge between two nodes\n",
    "        self.buffer_action_space = torch.full((self.N_NODES, self.N_NODES), 1).long()  \n",
    "        self.action_space = torch.full((self.N_NETWORKS,self.N_NODES, self.N_NODES), 1).long()  \n",
    "        self.new_edges = list(map(restructure_edges,network))\n",
    "        self.network_idx = torch.arange(self.N_NETWORKS, dtype=torch.long)\n",
    "        for n in range(self.N_NETWORKS):\n",
    "            source = torch.tensor(self.new_edges[n]['source_id']).long()\n",
    "            target = torch.tensor(self.new_edges[n]['target_id']).long()\n",
    "            reward = torch.tensor(self.new_edges[n]['reward']).long()\n",
    "            self.buffer_action_space[source,target]=reward\n",
    "            self.action_space[n,:,:] = self.buffer_action_space\n",
    "\n",
    "\n",
    "        # get adjacency matrix with reward index instead of actual reward\n",
    "        # 0 here means that no edge is present, all other indices from 1 to 5 indicate a reward\n",
    "        # (the higher the index number, the higher the reward)\n",
    "        # (using this solution for now: https://discuss.pytorch.org/t/mapping-values-in-a-tensor/117731)\n",
    "        self.action_space_idx = self.action_space.detach().clone()\n",
    "        self.action_space_idx.apply_(lambda val: self.possible_rewards.get(val, 0))\n",
    "        print(f'example of action space idx for 1 env: {self.action_space_idx[0,:,:]}')\n",
    "\n",
    "        # boolean adjacency matrix \n",
    "        self.edge_is_present = torch.squeeze(torch.unsqueeze(self.action_space!=1,dim=-1))\n",
    "        \n",
    "        #self.all_edges_source = [torch.where(self.action_space[i,:,:]!=1)[0] for i in range(self.N_NETWORKS)] \n",
    "        #self.all_edges_target = [torch.where(self.action_space[i,:,:]!=1)[1] for i in range(self.N_NETWORKS)] \n",
    "        \n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.reward_balance = torch.full((self.N_NETWORKS,1),self.INIT_REWARD)\n",
    "        self.step_counter = torch.full((self.N_NETWORKS,1),self.INIT_STEP)\n",
    "        self.is_done = False \n",
    "        self.current_node = self.starting_nodes.clone()\n",
    "        #self.current_node = torch.tensor(list(map(lambda n: n['starting_node'], self.network)), dtype=torch.long)\n",
    "\n",
    "        print('Environment initialized: \\n')\n",
    "        print(f'- set of nodes of shape {self.nodes.shape}')\n",
    "        print(f'- action space of shape {self.action_space.shape}')\n",
    "        print(f'- reward balance of shape {self.reward_balance.shape}')\n",
    "        print(f'- step counter of shape {self.step_counter.shape}')\n",
    "        print(f'- current node of shape {self.current_node.shape}')\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Take a step in all environments; here action corresponds to the target nodes for each env\n",
    "        action_i \\in [0,1,2,3,4,5,6,7,8,9]\n",
    "        '''\n",
    "\n",
    "        self.source_node = self.current_node\n",
    "        print(f'Source nodes are: {self.current_node}, we are going to new nodes {action}')\n",
    "\n",
    "        self.rewards = torch.unsqueeze(self.action_space[self.network_idx,self.current_node,action], dim=-1)\n",
    "\n",
    "        # TODO remove; only used to validate the equivalence during development\n",
    "        rewards_old = torch.unsqueeze( torch.tensor([self.action_space[i,self.current_node[i],action[i]] for i in range(self.N_NETWORKS)]), dim=-1)\n",
    "        torch.testing.assert_close(self.rewards, rewards_old)\n",
    "        # remove end\n",
    "\n",
    "        print(f'We get rewards : {self.rewards[:,0]}')\n",
    "        self.reward_balance = torch.add(self.reward_balance,self.rewards)\n",
    "        print(f'New reward balance is: {self.reward_balance[:,0]}')\n",
    "        self.current_node = action\n",
    "        print(f'Now we are in nodes: {self.current_node}')\n",
    "        self.step_counter = torch.add(self.step_counter,1)\n",
    "        print(f'Step counter for all networks is: {self.step_counter[:,0]}')\n",
    "        print('\\n')\n",
    "\n",
    "        if torch.all(self.step_counter == 8):\n",
    "            self.is_done = True\n",
    "        \n",
    "        \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        this function returns the current state of the environment.\n",
    "        State information given by this funciton is less detailed compared\n",
    "        to the observation. \n",
    "        \"\"\"\n",
    "        return {'current_node':self.current_node,\n",
    "                'total_reward':self.reward_balance,\n",
    "                'n_steps':self.step_counter,\n",
    "                'done':self.is_done}\n",
    "\n",
    "\n",
    "    def get_possible_rewards(self,obs):\n",
    "        \"\"\"\n",
    "        this function returns the next possible rewards given an observation;\n",
    "        the rewards are selected using boolean masking, and the resulting array is split\n",
    "        into sub-tensors whose size is given by how many valid edges are present in each network\n",
    "        in the current observation.\n",
    "        \"\"\"\n",
    "        self.n_rewards_per_network = torch.count_nonzero(obs['next_possible_nodes'],dim=1).tolist()\n",
    "        self.next_rewards_all = torch.masked_select(obs['next_possible_rewards'][self.network_idx],obs['next_possible_nodes'][self.network_idx])\n",
    "        self.next_rewards_per_network = torch.split(self.next_rewards_all,self.n_rewards_per_network)\n",
    "        \n",
    "        return self.next_rewards_per_network\n",
    "\n",
    "    def observe(self):\n",
    "        \"\"\"\n",
    "        TODO: CHANGE FOR VECTORIZATION\n",
    "        this function returns observation from the environment\n",
    "        \"\"\"\n",
    "        #self.valid_edges_source = [torch.where(self.all_edges_source[i]==self.current_node[i])[:] for i in range(self.N_NETWORKS)]\n",
    "        #self.valid_edges_target = [self.all_edges_target[i][self.valid_edges_source[i]] for i in range(self.N_NETWORKS)]\n",
    "        \n",
    "        self.next_nodes = torch.squeeze(torch.unsqueeze(self.edge_is_present[self.network_idx,self.current_node,:],dim=-1))\n",
    "        self.next_rewards = torch.squeeze(torch.unsqueeze(self.action_space[self.network_idx,self.current_node,:],dim=-1))\n",
    "        self.next_rewards_idx = torch.squeeze(torch.unsqueeze(self.action_space_idx[self.network_idx,self.current_node,:],dim=-1))\n",
    "\n",
    "        return {'current_node':self.current_node,\n",
    "                'next_possible_nodes':self.next_nodes,#torch.squeeze(torch.unsqueeze(self.next_nodes[self.network_idx,self.network_idx,:],dim=-1)) ,\n",
    "                'next_possible_rewards':self.next_rewards, #torch.squeeze(torch.unsqueeze(self.next_rewards[self.network_idx,self.network_idx,:],dim=-1)),\n",
    "                'next_possible_rewards_idx':self.next_rewards_idx,\n",
    "                'total_reward':self.reward_balance,\n",
    "                'n_steps':self.step_counter,\n",
    "                'done':self.is_done}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent that uses pytorch functions to solve multiple environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self,strategy: str):\n",
    "        \"\"\"\n",
    "        initialize agent\n",
    "        \"\"\"\n",
    "        assert strategy in ['random','highest_payoff'], f'a strategy {strategy} was given, but availabe strategies are {[\"random\",\"highest_payoff\",\"take_loss\"]}'\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def choose_action(self,obs):\n",
    "        \"\"\"\n",
    "        Choose next action given an observation and a strategy\n",
    "        \"\"\"\n",
    "\n",
    "        if self.strategy=='random':\n",
    "\n",
    "            # create a copy of the next actions index matrix, this will be modified to \n",
    "            # select random indices (random next action to take) with torch.multinomial\n",
    "            #self.next_actions_idx_p = obs['next_possible_rewards_idx'].detach().clone().to(torch.float32)\n",
    "            # specify a probability matrix, uniform distribution over all valid (not 0) indices for each network index\n",
    "            # (prob. associated to each valid action depends on the number of valid actions in each network)\n",
    "            #self.p_one = torch.tensor([1,1,1,1,1])\n",
    "            #self.p_divided = torch.divide(self.p_one,torch.count_nonzero(self.next_actions_idx_p,dim=1))\n",
    "            # find where there are valid actions in the matrix\n",
    "            #self.non_zero_indices = torch.argwhere(self.next_actions_idx_p) \n",
    "            # find specific indices of where valid actions are for each network index\n",
    "            #self.next_actions_idx_list = list(torch.split(self.non_zero_indices,\n",
    "            #                                              torch.count_nonzero(self.next_actions_idx_p,dim=1).tolist()))\n",
    "\n",
    "            # TO FIX! still uses for loop! How to improve this?\n",
    "            #for i in range(len(self.next_actions_idx_list)):\n",
    "            #    self.next_actions_idx_p[self.next_actions_idx_list[i][:,0],self.next_actions_idx_list[i][:,1]]=self.p_divided[i]\n",
    "            \n",
    "            self.next_action = torch.squeeze(torch.multinomial(obs['next_possible_nodes'].type(torch.float),1))\n",
    "            #self.next_action = torch.squeeze(torch.multinomial(self.next_actions_idx_p,1,replacement=False))\n",
    "            print(f'shape of next aciton in agent: {self.next_action.shape}')\n",
    "\n",
    "        elif self.strategy=='highest_payoff':\n",
    "            self.next_action = torch.argmax(obs['next_possible_rewards_idx'],dim=1)\n",
    "        \n",
    "        return self.next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load a few networks and test (with random policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=r\"../data\"\n",
    "with open(os.path.join(data_dir,'train.json')) as json_file:\n",
    "    train = json.load(json_file)\n",
    "\n",
    "test = train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of action space idx for 1 env: tensor([[0, 0, 2, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [4, 0, 0, 0, 4, 0, 0, 0, 0, 0],\n",
      "        [0, 3, 0, 2, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 4, 0, 0, 2, 0, 0, 0, 0, 0],\n",
      "        [2, 0, 4, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 3, 0, 0, 3, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 3, 4, 0],\n",
      "        [0, 0, 0, 0, 0, 2, 0, 0, 0, 3],\n",
      "        [0, 0, 0, 0, 0, 0, 4, 0, 0, 4],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 5, 5, 0]])\n",
      "Environment initialized: \n",
      "\n",
      "- set of nodes of shape torch.Size([5, 10])\n",
      "- action space of shape torch.Size([5, 10, 10])\n",
      "- reward balance of shape torch.Size([5, 1])\n",
      "- step counter of shape torch.Size([5, 1])\n",
      "- current node of shape torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "N = Reward_Network(test)\n",
    "A = Agent('random')\n",
    "N.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    1,  -20,    1,    1, -100,    1,    1,    1,    1],\n",
       "        [  20,    1,    1,    1,   20,    1,    1,    1,    1,    1],\n",
       "        [   1,    0,    1,  -20,    1,    1,    1,    1,    1,    1],\n",
       "        [   1,   20,    1,    1,  -20,    1,    1,    1,    1,    1],\n",
       "        [ -20,    1,   20,    1,    1,    1,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    0,    1,    1,    0,    1,    1,    1],\n",
       "        [   1,    1,    1,    1,    1,    1,    1,    0,   20,    1],\n",
       "        [   1,    1,    1,    1,    1,  -20,    1,    1,    1,    0],\n",
       "        [   1,    1,    1,    1,    1,    1,   20,    1,    1,   20],\n",
       "        [   1,    1,    1,    1,    1,    1,    1,  140,  140,    1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.action_space[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_node': tensor([0, 0, 0, 0, 0]),\n",
       " 'next_possible_nodes': tensor([[False, False,  True, False, False,  True, False, False, False, False],\n",
       "         [False,  True,  True,  True, False,  True, False, False, False, False],\n",
       "         [False,  True,  True,  True, False,  True, False, False, False, False],\n",
       "         [False,  True,  True,  True, False,  True,  True,  True, False, False],\n",
       "         [False,  True,  True,  True, False,  True,  True,  True, False, False]]),\n",
       " 'next_possible_rewards': tensor([[   1,    1,  -20,    1,    1, -100,    1,    1,    1,    1],\n",
       "         [   1,   20,  -20,   20,    1, -100,    1,    1,    1,    1],\n",
       "         [   1,   20,  -20,   20,    1, -100,    1,    1,    1,    1],\n",
       "         [   1,   20,  -20,   20,    1, -100, -100, -100,    1,    1],\n",
       "         [   1,   20,  -20,    0,    1, -100, -100, -100,    1,    1]]),\n",
       " 'next_possible_rewards_idx': tensor([[0, 0, 2, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 4, 2, 4, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 4, 2, 4, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 4, 2, 4, 0, 1, 1, 1, 0, 0],\n",
       "         [0, 4, 2, 3, 0, 1, 1, 1, 0, 0]]),\n",
       " 'total_reward': tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]),\n",
       " 'n_steps': tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]),\n",
       " 'done': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current node in the envs: tensor([0, 0, 0, 0, 0])\n",
      "---------------------------------------------\n",
      "current_node\n",
      "tensor([0, 0, 0, 0, 0])\n",
      "next_possible_nodes\n",
      "tensor([[False, False,  True, False, False,  True, False, False, False, False],\n",
      "        [False,  True,  True,  True, False,  True, False, False, False, False],\n",
      "        [False,  True,  True,  True, False,  True, False, False, False, False],\n",
      "        [False,  True,  True,  True, False,  True,  True,  True, False, False],\n",
      "        [False,  True,  True,  True, False,  True,  True,  True, False, False]])\n",
      "next_possible_rewards\n",
      "tensor([[   1,    1,  -20,    1,    1, -100,    1,    1,    1,    1],\n",
      "        [   1,   20,  -20,   20,    1, -100,    1,    1,    1,    1],\n",
      "        [   1,   20,  -20,   20,    1, -100,    1,    1,    1,    1],\n",
      "        [   1,   20,  -20,   20,    1, -100, -100, -100,    1,    1],\n",
      "        [   1,   20,  -20,    0,    1, -100, -100, -100,    1,    1]])\n",
      "next_possible_rewards_idx\n",
      "tensor([[0, 0, 2, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 4, 2, 4, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 4, 2, 4, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 4, 2, 4, 0, 1, 1, 1, 0, 0],\n",
      "        [0, 4, 2, 3, 0, 1, 1, 1, 0, 0]])\n",
      "total_reward\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "n_steps\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "done\n",
      "False\n",
      "\n",
      "\n",
      "possible rewards that we can get in next step: \n",
      "\n",
      "(tensor([ -20, -100]), tensor([  20,  -20,   20, -100]), tensor([  20,  -20,   20, -100]), tensor([  20,  -20,   20, -100, -100, -100]), tensor([  20,  -20,    0, -100, -100, -100]))\n",
      "\n",
      "\n",
      "shape of next aciton in agent: torch.Size([5])\n",
      "Next actions chosen: tensor([5, 2, 3, 5, 3])\n",
      "Source nodes are: tensor([0, 0, 0, 0, 0]), we are going to new nodes tensor([5, 2, 3, 5, 3])\n",
      "We get rewards : tensor([-100,  -20,   20, -100,    0])\n",
      "New reward balance is: tensor([-100,  -20,   20, -100,    0])\n",
      "Now we are in nodes: tensor([5, 2, 3, 5, 3])\n",
      "Step counter for all networks is: tensor([1, 1, 1, 1, 1])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([5, 2, 3, 5, 3])\n",
      "---------------------------------------------\n",
      "current_node\n",
      "tensor([5, 2, 3, 5, 3])\n",
      "next_possible_nodes\n",
      "tensor([[False, False, False,  True, False, False,  True, False, False, False],\n",
      "        [False,  True, False,  True, False,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False,  True,  True, False, False, False, False],\n",
      "        [ True, False, False,  True, False, False,  True, False,  True, False],\n",
      "        [ True,  True,  True, False,  True,  True, False, False, False, False]])\n",
      "next_possible_rewards\n",
      "tensor([[   1,    1,    1,    0,    1,    1,    0,    1,    1,    1],\n",
      "        [   1,    0,    1,  -20,    1, -100,    1,    1,    1,    1],\n",
      "        [   0,   20,   20,    1, -100, -100,    1,    1,    1,    1],\n",
      "        [ -20,    1,    1,    0,    1,    1,   20,    1,   20,    1],\n",
      "        [   0,   20,  -20,    1,  -20, -100,    1,    1,    1,    1]])\n",
      "next_possible_rewards_idx\n",
      "tensor([[0, 0, 0, 3, 0, 0, 3, 0, 0, 0],\n",
      "        [0, 3, 0, 2, 0, 1, 0, 0, 0, 0],\n",
      "        [3, 4, 4, 0, 1, 1, 0, 0, 0, 0],\n",
      "        [2, 0, 0, 3, 0, 0, 4, 0, 4, 0],\n",
      "        [3, 4, 2, 0, 2, 1, 0, 0, 0, 0]])\n",
      "total_reward\n",
      "tensor([[-100],\n",
      "        [ -20],\n",
      "        [  20],\n",
      "        [-100],\n",
      "        [   0]])\n",
      "n_steps\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n",
      "done\n",
      "False\n",
      "\n",
      "\n",
      "possible rewards that we can get in next step: \n",
      "\n",
      "(tensor([0, 0]), tensor([   0,  -20, -100]), tensor([   0,   20,   20, -100, -100]), tensor([-20,   0,  20,  20]), tensor([   0,   20,  -20,  -20, -100]))\n",
      "\n",
      "\n",
      "shape of next aciton in agent: torch.Size([5])\n",
      "Next actions chosen: tensor([6, 5, 0, 6, 0])\n",
      "Source nodes are: tensor([5, 2, 3, 5, 3]), we are going to new nodes tensor([6, 5, 0, 6, 0])\n",
      "We get rewards : tensor([   0, -100,    0,   20,    0])\n",
      "New reward balance is: tensor([-100, -120,   20,  -80,    0])\n",
      "Now we are in nodes: tensor([6, 5, 0, 6, 0])\n",
      "Step counter for all networks is: tensor([2, 2, 2, 2, 2])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([6, 5, 0, 6, 0])\n",
      "---------------------------------------------\n",
      "current_node\n",
      "tensor([6, 5, 0, 6, 0])\n",
      "next_possible_nodes\n",
      "tensor([[False, False, False, False, False, False, False,  True,  True, False],\n",
      "        [ True, False, False,  True, False, False,  True, False, False, False],\n",
      "        [False,  True,  True,  True, False,  True, False, False, False, False],\n",
      "        [ True, False, False,  True, False, False, False,  True,  True,  True],\n",
      "        [False,  True,  True,  True, False,  True,  True,  True, False, False]])\n",
      "next_possible_rewards\n",
      "tensor([[   1,    1,    1,    1,    1,    1,    1,    0,   20,    1],\n",
      "        [ -20,    1,    1,    0,    1,    1,    0,    1,    1,    1],\n",
      "        [   1,   20,  -20,   20,    1, -100,    1,    1,    1,    1],\n",
      "        [ -20,    1,    1,    0,    1,    1,    1,   20,   20,    0],\n",
      "        [   1,   20,  -20,    0,    1, -100, -100, -100,    1,    1]])\n",
      "next_possible_rewards_idx\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 3, 4, 0],\n",
      "        [2, 0, 0, 3, 0, 0, 3, 0, 0, 0],\n",
      "        [0, 4, 2, 4, 0, 1, 0, 0, 0, 0],\n",
      "        [2, 0, 0, 3, 0, 0, 0, 4, 4, 3],\n",
      "        [0, 4, 2, 3, 0, 1, 1, 1, 0, 0]])\n",
      "total_reward\n",
      "tensor([[-100],\n",
      "        [-120],\n",
      "        [  20],\n",
      "        [ -80],\n",
      "        [   0]])\n",
      "n_steps\n",
      "tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]])\n",
      "done\n",
      "False\n",
      "\n",
      "\n",
      "possible rewards that we can get in next step: \n",
      "\n",
      "(tensor([ 0, 20]), tensor([-20,   0,   0]), tensor([  20,  -20,   20, -100]), tensor([-20,   0,  20,  20,   0]), tensor([  20,  -20,    0, -100, -100, -100]))\n",
      "\n",
      "\n",
      "shape of next aciton in agent: torch.Size([5])\n",
      "Next actions chosen: tensor([8, 3, 1, 3, 7])\n",
      "Source nodes are: tensor([6, 5, 0, 6, 0]), we are going to new nodes tensor([8, 3, 1, 3, 7])\n",
      "We get rewards : tensor([  20,    0,   20,    0, -100])\n",
      "New reward balance is: tensor([ -80, -120,   40,  -80, -100])\n",
      "Now we are in nodes: tensor([8, 3, 1, 3, 7])\n",
      "Step counter for all networks is: tensor([3, 3, 3, 3, 3])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([8, 3, 1, 3, 7])\n",
      "---------------------------------------------\n",
      "current_node\n",
      "tensor([8, 3, 1, 3, 7])\n",
      "next_possible_nodes\n",
      "tensor([[False, False, False, False, False, False,  True, False, False,  True],\n",
      "        [ True,  True, False, False,  True,  True, False, False, False, False],\n",
      "        [ True, False,  True, False,  True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False,  True,  True, False, False, False, False],\n",
      "        [False, False, False, False,  True,  True,  True, False,  True,  True]])\n",
      "next_possible_rewards\n",
      "tensor([[   1,    1,    1,    1,    1,    1,   20,    1,    1,   20],\n",
      "        [   0,   20,    1,    1,  -20, -100,    1,    1,    1,    1],\n",
      "        [   0,    1,   20,    1, -100,    1,    1,    1,    1,    1],\n",
      "        [   0,   20,  -20,    1, -100, -100,    1,    1,    1,    1],\n",
      "        [   1,    1,    1,    1,    0,   20,   20,    1,   20,    0]])\n",
      "next_possible_rewards_idx\n",
      "tensor([[0, 0, 0, 0, 0, 0, 4, 0, 0, 4],\n",
      "        [3, 4, 0, 0, 2, 1, 0, 0, 0, 0],\n",
      "        [3, 0, 4, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [3, 4, 2, 0, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 3, 4, 4, 0, 4, 3]])\n",
      "total_reward\n",
      "tensor([[ -80],\n",
      "        [-120],\n",
      "        [  40],\n",
      "        [ -80],\n",
      "        [-100]])\n",
      "n_steps\n",
      "tensor([[3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3]])\n",
      "done\n",
      "False\n",
      "\n",
      "\n",
      "possible rewards that we can get in next step: \n",
      "\n",
      "(tensor([20, 20]), tensor([   0,   20,  -20, -100]), tensor([   0,   20, -100]), tensor([   0,   20,  -20, -100, -100]), tensor([ 0, 20, 20, 20,  0]))\n",
      "\n",
      "\n",
      "shape of next aciton in agent: torch.Size([5])\n",
      "Next actions chosen: tensor([6, 5, 4, 5, 5])\n",
      "Source nodes are: tensor([8, 3, 1, 3, 7]), we are going to new nodes tensor([6, 5, 4, 5, 5])\n",
      "We get rewards : tensor([  20, -100, -100, -100,   20])\n",
      "New reward balance is: tensor([ -60, -220,  -60, -180,  -80])\n",
      "Now we are in nodes: tensor([6, 5, 4, 5, 5])\n",
      "Step counter for all networks is: tensor([4, 4, 4, 4, 4])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([6, 5, 4, 5, 5])\n",
      "---------------------------------------------\n",
      "current_node\n",
      "tensor([6, 5, 4, 5, 5])\n",
      "next_possible_nodes\n",
      "tensor([[False, False, False, False, False, False, False,  True,  True, False],\n",
      "        [ True, False, False,  True, False, False,  True, False, False, False],\n",
      "        [ True, False,  True,  True, False,  True, False,  True, False, False],\n",
      "        [ True, False, False,  True, False, False,  True, False,  True, False],\n",
      "        [ True, False, False,  True, False, False,  True, False,  True, False]])\n",
      "next_possible_rewards\n",
      "tensor([[  1,   1,   1,   1,   1,   1,   1,   0,  20,   1],\n",
      "        [-20,   1,   1,   0,   1,   1,   0,   1,   1,   1],\n",
      "        [-20,   1,  20, -20,   1,  20,   1,  20,   1,   1],\n",
      "        [-20,   1,   1,   0,   1,   1,  20,   1,  20,   1],\n",
      "        [-20,   1,   1,   0,   1,   1,  20,   1,  20,   1]])\n",
      "next_possible_rewards_idx\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 3, 4, 0],\n",
      "        [2, 0, 0, 3, 0, 0, 3, 0, 0, 0],\n",
      "        [2, 0, 4, 2, 0, 4, 0, 4, 0, 0],\n",
      "        [2, 0, 0, 3, 0, 0, 4, 0, 4, 0],\n",
      "        [2, 0, 0, 3, 0, 0, 4, 0, 4, 0]])\n",
      "total_reward\n",
      "tensor([[ -60],\n",
      "        [-220],\n",
      "        [ -60],\n",
      "        [-180],\n",
      "        [ -80]])\n",
      "n_steps\n",
      "tensor([[4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4]])\n",
      "done\n",
      "False\n",
      "\n",
      "\n",
      "possible rewards that we can get in next step: \n",
      "\n",
      "(tensor([ 0, 20]), tensor([-20,   0,   0]), tensor([-20,  20, -20,  20,  20]), tensor([-20,   0,  20,  20]), tensor([-20,   0,  20,  20]))\n",
      "\n",
      "\n",
      "shape of next aciton in agent: torch.Size([5])\n",
      "Next actions chosen: tensor([8, 0, 3, 0, 3])\n",
      "Source nodes are: tensor([6, 5, 4, 5, 5]), we are going to new nodes tensor([8, 0, 3, 0, 3])\n",
      "We get rewards : tensor([ 20, -20, -20, -20,   0])\n",
      "New reward balance is: tensor([ -40, -240,  -80, -200,  -80])\n",
      "Now we are in nodes: tensor([8, 0, 3, 0, 3])\n",
      "Step counter for all networks is: tensor([5, 5, 5, 5, 5])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([8, 0, 3, 0, 3])\n",
      "---------------------------------------------\n",
      "current_node\n",
      "tensor([8, 0, 3, 0, 3])\n",
      "next_possible_nodes\n",
      "tensor([[False, False, False, False, False, False,  True, False, False,  True],\n",
      "        [False,  True,  True,  True, False,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False,  True,  True, False, False, False, False],\n",
      "        [False,  True,  True,  True, False,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False,  True,  True, False, False, False, False]])\n",
      "next_possible_rewards\n",
      "tensor([[   1,    1,    1,    1,    1,    1,   20,    1,    1,   20],\n",
      "        [   1,   20,  -20,   20,    1, -100,    1,    1,    1,    1],\n",
      "        [   0,   20,   20,    1, -100, -100,    1,    1,    1,    1],\n",
      "        [   1,   20,  -20,   20,    1, -100, -100, -100,    1,    1],\n",
      "        [   0,   20,  -20,    1,  -20, -100,    1,    1,    1,    1]])\n",
      "next_possible_rewards_idx\n",
      "tensor([[0, 0, 0, 0, 0, 0, 4, 0, 0, 4],\n",
      "        [0, 4, 2, 4, 0, 1, 0, 0, 0, 0],\n",
      "        [3, 4, 4, 0, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 4, 2, 4, 0, 1, 1, 1, 0, 0],\n",
      "        [3, 4, 2, 0, 2, 1, 0, 0, 0, 0]])\n",
      "total_reward\n",
      "tensor([[ -40],\n",
      "        [-240],\n",
      "        [ -80],\n",
      "        [-200],\n",
      "        [ -80]])\n",
      "n_steps\n",
      "tensor([[5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5]])\n",
      "done\n",
      "False\n",
      "\n",
      "\n",
      "possible rewards that we can get in next step: \n",
      "\n",
      "(tensor([20, 20]), tensor([  20,  -20,   20, -100]), tensor([   0,   20,   20, -100, -100]), tensor([  20,  -20,   20, -100, -100, -100]), tensor([   0,   20,  -20,  -20, -100]))\n",
      "\n",
      "\n",
      "shape of next aciton in agent: torch.Size([5])\n",
      "Next actions chosen: tensor([9, 2, 0, 6, 1])\n",
      "Source nodes are: tensor([8, 0, 3, 0, 3]), we are going to new nodes tensor([9, 2, 0, 6, 1])\n",
      "We get rewards : tensor([  20,  -20,    0, -100,   20])\n",
      "New reward balance is: tensor([ -20, -260,  -80, -300,  -60])\n",
      "Now we are in nodes: tensor([9, 2, 0, 6, 1])\n",
      "Step counter for all networks is: tensor([6, 6, 6, 6, 6])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([9, 2, 0, 6, 1])\n",
      "---------------------------------------------\n",
      "current_node\n",
      "tensor([9, 2, 0, 6, 1])\n",
      "next_possible_nodes\n",
      "tensor([[False, False, False, False, False, False, False,  True,  True, False],\n",
      "        [False,  True, False,  True, False,  True, False, False, False, False],\n",
      "        [False,  True,  True,  True, False,  True, False, False, False, False],\n",
      "        [ True, False, False,  True, False, False, False,  True,  True,  True],\n",
      "        [ True, False,  True,  True,  True,  True, False, False, False, False]])\n",
      "next_possible_rewards\n",
      "tensor([[   1,    1,    1,    1,    1,    1,    1,  140,  140,    1],\n",
      "        [   1,    0,    1,  -20,    1, -100,    1,    1,    1,    1],\n",
      "        [   1,   20,  -20,   20,    1, -100,    1,    1,    1,    1],\n",
      "        [ -20,    1,    1,    0,    1,    1,    1,   20,   20,    0],\n",
      "        [   0,    1,  -20,    0, -100, -100,    1,    1,    1,    1]])\n",
      "next_possible_rewards_idx\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 5, 5, 0],\n",
      "        [0, 3, 0, 2, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 4, 2, 4, 0, 1, 0, 0, 0, 0],\n",
      "        [2, 0, 0, 3, 0, 0, 0, 4, 4, 3],\n",
      "        [3, 0, 2, 3, 1, 1, 0, 0, 0, 0]])\n",
      "total_reward\n",
      "tensor([[ -20],\n",
      "        [-260],\n",
      "        [ -80],\n",
      "        [-300],\n",
      "        [ -60]])\n",
      "n_steps\n",
      "tensor([[6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6]])\n",
      "done\n",
      "False\n",
      "\n",
      "\n",
      "possible rewards that we can get in next step: \n",
      "\n",
      "(tensor([140, 140]), tensor([   0,  -20, -100]), tensor([  20,  -20,   20, -100]), tensor([-20,   0,  20,  20,   0]), tensor([   0,  -20,    0, -100, -100]))\n",
      "\n",
      "\n",
      "shape of next aciton in agent: torch.Size([5])\n",
      "Next actions chosen: tensor([7, 3, 5, 9, 4])\n",
      "Source nodes are: tensor([9, 2, 0, 6, 1]), we are going to new nodes tensor([7, 3, 5, 9, 4])\n",
      "We get rewards : tensor([ 140,  -20, -100,    0, -100])\n",
      "New reward balance is: tensor([ 120, -280, -180, -300, -160])\n",
      "Now we are in nodes: tensor([7, 3, 5, 9, 4])\n",
      "Step counter for all networks is: tensor([7, 7, 7, 7, 7])\n",
      "\n",
      "\n",
      "Current node in the envs: tensor([7, 3, 5, 9, 4])\n",
      "---------------------------------------------\n",
      "current_node\n",
      "tensor([7, 3, 5, 9, 4])\n",
      "next_possible_nodes\n",
      "tensor([[False, False, False, False, False,  True, False, False, False,  True],\n",
      "        [ True,  True, False, False,  True,  True, False, False, False, False],\n",
      "        [ True, False, False,  True, False, False,  True, False,  True, False],\n",
      "        [False, False,  True,  True,  True, False, False,  True,  True, False],\n",
      "        [ True, False,  True,  True, False,  True, False,  True,  True, False]])\n",
      "next_possible_rewards\n",
      "tensor([[   1,    1,    1,    1,    1,  -20,    1,    1,    1,    0],\n",
      "        [   0,   20,    1,    1,  -20, -100,    1,    1,    1,    1],\n",
      "        [ -20,    1,    1,    0,    1,    1,    0,    1,   20,    1],\n",
      "        [   1,    1,  140,   20,  140,    1,    1,  140,  140,    1],\n",
      "        [ -20,    1,    0,   20,    1,   20,    1,   20,    0,    1]])\n",
      "next_possible_rewards_idx\n",
      "tensor([[0, 0, 0, 0, 0, 2, 0, 0, 0, 3],\n",
      "        [3, 4, 0, 0, 2, 1, 0, 0, 0, 0],\n",
      "        [2, 0, 0, 3, 0, 0, 3, 0, 4, 0],\n",
      "        [0, 0, 5, 4, 5, 0, 0, 5, 5, 0],\n",
      "        [2, 0, 3, 4, 0, 4, 0, 4, 3, 0]])\n",
      "total_reward\n",
      "tensor([[ 120],\n",
      "        [-280],\n",
      "        [-180],\n",
      "        [-300],\n",
      "        [-160]])\n",
      "n_steps\n",
      "tensor([[7],\n",
      "        [7],\n",
      "        [7],\n",
      "        [7],\n",
      "        [7]])\n",
      "done\n",
      "False\n",
      "\n",
      "\n",
      "possible rewards that we can get in next step: \n",
      "\n",
      "(tensor([-20,   0]), tensor([   0,   20,  -20, -100]), tensor([-20,   0,   0,  20]), tensor([140,  20, 140, 140, 140]), tensor([-20,   0,  20,  20,  20,   0]))\n",
      "\n",
      "\n",
      "shape of next aciton in agent: torch.Size([5])\n",
      "Next actions chosen: tensor([5, 1, 3, 8, 0])\n",
      "Source nodes are: tensor([7, 3, 5, 9, 4]), we are going to new nodes tensor([5, 1, 3, 8, 0])\n",
      "We get rewards : tensor([-20,  20,   0, 140, -20])\n",
      "New reward balance is: tensor([ 100, -260, -180, -160, -180])\n",
      "Now we are in nodes: tensor([5, 1, 3, 8, 0])\n",
      "Step counter for all networks is: tensor([8, 8, 8, 8, 8])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NEW: Using the observation to step in the environment\n",
    "while N.is_done==False:\n",
    "    print(f'Current node in the envs: {N.current_node}')\n",
    "    print('---------------------------------------------')\n",
    "\n",
    "    obs = N.observe()\n",
    "    for key, value in obs.items():\n",
    "        print(key)\n",
    "        print(value)\n",
    "    print('\\n')\n",
    "\n",
    "    possible_rewards = N.get_possible_rewards(obs)\n",
    "    print(f'possible rewards that we can get in next step: \\n')\n",
    "    print(possible_rewards)\n",
    "    print('\\n')\n",
    "    \n",
    "    # select action based on observation\n",
    "    next_action = A.choose_action(obs)\n",
    "    print(f'Next actions chosen: {next_action}')\n",
    "    # take a step in envs\n",
    "    N.step(next_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Gym.Spaces to define observation space of the environment in order to use stable-baselines3?\n",
    "\n",
    "https://stackoverflow.com/questions/56448260/how-could-i-define-the-observation-space-for-my-custom-openai-enviroment\n",
    "https://github.com/openai/gym/blob/master/gym/spaces/graph.py\n",
    "https://github.com/openai/gym/issues/2912\n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html (current version of env does not completely follow Gym API since we custom define the action and observation space without using the gym.spaces submodule)\n",
    "https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html\n",
    "\n",
    "\n",
    "The problem with the openai gym enviroments is that their space wrappers generate the action space and observation space but there doesn't seem to be much room for customization\n",
    "\n",
    "A possibility could be to define `Tuple(Discrete(10),Discrete(10))` or `MultiDiscrete ([10,10])` for the action space, and a utility function to check if actions are valid in the current node you're at.\n",
    "\n",
    "Observation space might be a bit trickier, should include at least current node, possible actions, step counter and done flag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7535b6583ac7e43eef2c602d4cb77470ba1eeca741c11a0535b54e0aa8791c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
