{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \"\"\"Storage for observation of a DQN agent.\n",
    "\n",
    "    Observations are stored large continuous tensor.\n",
    "    The tensor are automatically initialized upon the first call of store().\n",
    "    Important: all tensors to be stored need to be passed at the first call of\n",
    "    the store. Also the shape of tensors to be stored needs to be consistent.\n",
    "\n",
    "\n",
    "    Typical usage:\n",
    "        mem = Memory(...)\n",
    "        for episode in range(n_episodes):\n",
    "            obs = env.init()\n",
    "            for round in range(n_rounds):\n",
    "                action = agent(obs)\n",
    "                next_obs, reward = env.step()\n",
    "                mem.store(**obs, reward=reward, action=action)\n",
    "                obs = next_obs\n",
    "            mem.finish_episode()\n",
    "\n",
    "            sample = mem.sample()\n",
    "            update_agents(sample)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, device, size, n_rounds):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                device: device for the memory\n",
    "                size: number of episode to store\n",
    "                n_rounds; number of rounds to store per episode\n",
    "        \"\"\"\n",
    "        self.memory = None\n",
    "        self.size = size\n",
    "        self.n_rounds = n_rounds\n",
    "        self.device = device\n",
    "        self.current_row = 0\n",
    "        self.episodes_stored = 0\n",
    "\n",
    "    def init_store(self, state):\n",
    "        \"\"\"Initialize the memory tensor.\n",
    "        \"\"\"\n",
    "        self.memory = {\n",
    "            k: th.zeros((self.size, self.n_rounds, *t.shape),\n",
    "                        dtype=t.dtype, device=self.device)\n",
    "            for k, t in state.items() if t is not None\n",
    "        }\n",
    "\n",
    "    def finish_episode(self):\n",
    "        \"\"\"Moves the currently active slice in memory to the next episode.\n",
    "        \"\"\"\n",
    "        self.episodes_stored += 1\n",
    "        self.current_row = (self.current_row + 1) % self.size\n",
    "\n",
    "    def store(self, round, **state):\n",
    "        \"\"\"Stores multiple tensor in the memory.\n",
    "        \"\"\"\n",
    "        if self.memory is None:\n",
    "            self.init_store(state)\n",
    "        for k, t in state.items():\n",
    "            if t is not None:\n",
    "                self.memory[k][self.current_row, round] = t.to(self.device)\n",
    "\n",
    "    def sample(self, batch_size, device, **kwargs):\n",
    "        \"\"\"Samples form the memory.\n",
    "\n",
    "        Returns:\n",
    "            dict | None: Dict being stored. If the batch size is larger than the number\n",
    "            of episodes stored 'None' is returned.\n",
    "        \"\"\"\n",
    "        if len(self) < batch_size:\n",
    "            return None\n",
    "        random_memory_idx = th.randperm(len(self))[:batch_size]\n",
    "        print(f'random_memory_idx', random_memory_idx)\n",
    "        sample = {k: v[random_memory_idx].to(device) for k, v in self.memory.items()}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The current memory usage, i.e. the number of valid episodes in\n",
    "        the memory.This increases as episodes are added to the memory until the\n",
    "        maximum size of the memory is reached.\n",
    "        \"\"\"\n",
    "        return min(self.episodes_stored, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip epsiode 0\n",
      "Skip epsiode 1\n",
      "Skip epsiode 2\n",
      "random_memory_idx tensor([3, 0, 2, 1])\n",
      "mask torch.Size([4, 3, 2, 5])\n",
      "obs torch.Size([4, 3, 2, 5, 7])\n",
      "random_memory_idx tensor([3, 0, 2, 4])\n",
      "mask torch.Size([4, 3, 2, 5])\n",
      "obs torch.Size([4, 3, 2, 5, 7])\n",
      "random_memory_idx tensor([3, 1, 4, 0])\n",
      "mask torch.Size([4, 3, 2, 5])\n",
      "obs torch.Size([4, 3, 2, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 6\n",
    "n_rounds = 3\n",
    "n_networks = 2\n",
    "n_nodes = 5\n",
    "obs_shape = 7\n",
    "batch_size = 4\n",
    "device = th.device('cpu')\n",
    "\n",
    "mem = Memory(device=device, size=5, n_rounds=n_rounds)\n",
    "for epsiode in range(n_episodes):\n",
    "    for round in range(n_rounds):\n",
    "        # mock of the environment\n",
    "        obs = {\n",
    "            'mask': th.rand((n_networks,n_nodes)) > 0.8,\n",
    "            'obs': (th.rand((n_networks,n_nodes, obs_shape)) > 0.9).type(th.int64)\n",
    "        }\n",
    "        mem.store(round, **obs)\n",
    "    mem.finish_episode()\n",
    "    sample = mem.sample(batch_size, device=device)\n",
    "    if sample is not None:\n",
    "        for k,v in sample.items():\n",
    "            print(k, v.shape)\n",
    "    else:\n",
    "        print(f\"Skip epsiode {epsiode}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb69f0ef4544e39eb337edf93623ba0bcb5d33c8507c25d3a5365281a0bb2ca5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
