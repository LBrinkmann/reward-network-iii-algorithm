{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network agent with GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other reference implementations\n",
    "* https://github.com/keep9oing/DRQN-Pytorch-CartPole-v1/blob/main/DRQN.py (pytorch)\n",
    "* https://github.com/marload/DeepRL-TensorFlow2/blob/master/DRQN/DRQN_Discrete.py\n",
    "(clean, tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch as th\n",
    "\n",
    "\n",
    "class GRUAgent(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(GRUAgent, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        self.rnn = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_size, out_features=output_size)\n",
    "\n",
    "    def reset(self):\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = F.relu(self.linear1(obs))\n",
    "        h, self.hidden = self.rnn(x, self.hidden)\n",
    "        q = self.linear2(h)\n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN():\n",
    "    def __init__(\n",
    "            self, *, gamma=None, target_update_freq=None, model_args=None, opt_args=None, eps=None, policy_model=None):\n",
    "        if policy_model is None:\n",
    "            self.policy_model = Model(**model_args) # Here goes the Neural Network \n",
    "            self.target_model = Model(**model_args) # Here goes the Neural Network \n",
    "\n",
    "            self.target_model.eval()\n",
    "            self.optimizer = th.optim.RMSprop(self.policy_model.parameters(), **opt_args)\n",
    "            self.gamma = gamma\n",
    "            self.target_update_freq = target_update_freq\n",
    "            self.eps = eps\n",
    "            self.trainable = True\n",
    "        else:\n",
    "            self.policy_model = policy_model\n",
    "            self.trainable = False\n",
    "\n",
    "    def get_q(self, obs, first=False):\n",
    "        with th.no_grad():\n",
    "            return self.policy_model(obs, reset_rnn=first)\n",
    "\n",
    "    def eps_greedy(self, q_values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q_values: Tensor of type `th.float` and arbitrary shape, last dimension reflect the actions.\n",
    "        Returns:\n",
    "            actions: Tensor of type `th.long` and the same dimensions then q_values, besides of the last.\n",
    "        \"\"\"\n",
    "        assert self.trainable, \"Model is not trainable.\"\n",
    "        n_actions = q_values.shape[-1]\n",
    "        actions_shape = q_values.shape[:-1]\n",
    "\n",
    "        greedy_actions = q_values.argmax(-1)\n",
    "        random_actions = th.randint(0, n_actions, size=actions_shape, device=self.device)\n",
    "\n",
    "        # random number which determine whether to take the random action\n",
    "        random_numbers = th.rand(size=actions_shape, device=self.device)\n",
    "        select_random = (random_numbers < self.eps).long()\n",
    "        picked_actions = select_random * random_actions + (1 - select_random) * greedy_actions\n",
    "\n",
    "        return picked_actions\n",
    "\n",
    "    def update(self, update_step, action, reward, **obs):\n",
    "        assert self.trainable, \"Model is not trainable.\"\n",
    "        if (update_step % self.target_update_freq == 0):\n",
    "            # copy policy net to target net\n",
    "            self.target_model.load_state_dict(self.policy_model.state_dict())\n",
    "\n",
    "        self.policy_model.train()\n",
    "        current_state_action_values = self.policy_model(\n",
    "            obs, reset_rnn=True).gather(-1, action.unsqueeze(-1))\n",
    "\n",
    "        next_state_values = th.zeros_like(reward, device=self.device)\n",
    "\n",
    "        # we skip the first observation and set the future value for the terminal\n",
    "        # state to 0\n",
    "        next_state_values[:, :-1] \\\n",
    "            = self.target_model(obs, reset_rnn=True)[:, 1:].max(-1)[0].detach()\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward\n",
    "\n",
    "        loss_ur = th.nn.functional.smooth_l1_loss(\n",
    "            current_state_action_values, expected_state_action_values.unsqueeze(-1),\n",
    "            reduction='none')\n",
    "        loss_ur = loss_ur.mean(dim=0)\n",
    "\n",
    "        loss = loss_ur.mean()\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # truncate large loss\n",
    "        for param in self.policy_model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        return loss_ur\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        to_save = {\n",
    "            'policy_model': self.policy_model\n",
    "        }\n",
    "        th.save(to_save, filename)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename, device):\n",
    "        to_load = th.load(filename)\n",
    "        ah = cls(**to_load, device=device)\n",
    "        return ah\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "class Memory():\n",
    "    def __init__(\n",
    "            self, device, n_batches, n_rounds, batch_size, output_file=None):\n",
    "        self.memory = None\n",
    "        self.n_batches = n_batches\n",
    "        self.size = n_batches * batch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_rounds = n_rounds\n",
    "        self.device = device\n",
    "        self.output_file = output_file\n",
    "        self.start_row = None\n",
    "        self.end_row = None\n",
    "        self.rewind = 0\n",
    "        self.deque = collections.deque([], maxlen=self.n_batches*batch_size)\n",
    "\n",
    "    @property\n",
    "    def last_valid_row(self):\n",
    "        lvr = self.start_row + self.batch_size\n",
    "        return lvr if lvr >= 0 else None\n",
    "\n",
    "    def init_store(self, state):\n",
    "        self.memory = {\n",
    "            k: th.zeros((self.size, self.n_rounds, *t.shape[2:]),\n",
    "                        dtype=t.dtype, device=self.device)\n",
    "            for k, t in state.items() if t is not None\n",
    "        }\n",
    "\n",
    "    def start_batch(self):\n",
    "        if (self.end_row is None):\n",
    "            self.start_row = 0\n",
    "        elif (self.end_row + self.batch_size) >= self.size:\n",
    "            self.write()\n",
    "            self.rewind += 1\n",
    "            self.start_row = 0\n",
    "        else:\n",
    "            self.start_row = self.end_row\n",
    "        self.end_row = self.start_row + self.batch_size\n",
    "        self.current_batch = list(range(self.start_row, self.end_row))\n",
    "\n",
    "    def finish_batch(self):\n",
    "        self.deque.extendleft(self.current_batch)\n",
    "\n",
    "    def add(self, round_number, **state):\n",
    "        if self.memory is None:\n",
    "            self.init_store(state)\n",
    "\n",
    "        for k, t in state.items():\n",
    "            if t is not None:\n",
    "                self.memory[k][self.start_row:self.end_row, round_number] = t[:, 0].to(self.device)\n",
    "\n",
    "    def sample(self, **kwargs):\n",
    "        assert self.batch_size is not None, 'No sample size defined.'\n",
    "        if len(self) < self.batch_size:\n",
    "            return None, None\n",
    "        relative_episode = np.random.choice(len(self), self.batch_size, replace=False)\n",
    "        return self.get_relative(relative_episode, **kwargs)\n",
    "\n",
    "    def get_relative(self, relative_episode, keys=None, device=None):\n",
    "        if keys is None:\n",
    "            keys = self.memory.keys()\n",
    "        hist_idx = th.tensor(self.deque[relative_episode], dtype=th.int64, device=self.device)\n",
    "\n",
    "        sample = {k: v[hist_idx] for k, v in self.memory.items() if k in keys}\n",
    "        if device is not None:\n",
    "            sample = {k: v.to(device) for k, v in sample.items()}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.deque)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling of one batch of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "def run_batch(env, controller, replay_mem=None, on_policy=True, update_step=None):\n",
    "\n",
    "    obs = env.reset()\n",
    "    metric_list = []\n",
    "    for round_number in count():      \n",
    "        encoded = encode_observations(obs) # TODO\n",
    "\n",
    "        # Get q values from controller\n",
    "        q_values = controller.get_q(encoded, first=round_number == 0)\n",
    "    \n",
    "        if on_policy:\n",
    "            actions = q_values.argmax(-1)\n",
    "        else:\n",
    "            # Sample a action\n",
    "            actions = controller.eps_greedy(q_values=q_values)\n",
    "        prev_obs = obs\n",
    "        obs, rewards, done, info = env.step(actions)\n",
    "        \n",
    "        if replay_mem is not None:\n",
    "            replay_mem.add(\n",
    "                round_number=round_number, actions=actions, rewards=rewards, \n",
    "                prev_obs=prev_obs)\n",
    "        metrics = dict(\n",
    "            **info,\n",
    "            rewards=rewards.mean().item(),\n",
    "            q_min=q_values.min().item(),\n",
    "            q_max=q_values.max().item(),\n",
    "            q_mean=q_values.mean().item(),\n",
    "            round_number=round_number,\n",
    "            sampling='greedy' if on_policy else 'eps-greedy',\n",
    "            update_step=update_step,\n",
    "        )\n",
    "        metric_list.append(metrics)\n",
    "         \n",
    "        if done:\n",
    "            break\n",
    "    return metric_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_period = ...\n",
    "n_update_steps = ...\n",
    "device = ...\n",
    "\n",
    "device = th.device(device)\n",
    "cpu = th.device('cpu')\n",
    "\n",
    "env = ...\n",
    "\n",
    "controller = ...\n",
    "\n",
    "replay_mem = Memory(...)\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for update_step in range(n_update_steps):\n",
    "    replay_mem.start_batch(env.groups)\n",
    "\n",
    "    # here we sample one batch of episodes and add them to the replay buffer\n",
    "    off_policy_metrics = run_batch(env, controller, replay_mem, on_policy=False, update_step=update_step)\n",
    "\n",
    "    replay_mem.finish_batch()\n",
    "    \n",
    "    # allow controller to update itself\n",
    "    sample = replay_mem.sample(device=device)\n",
    "\n",
    "    if sample is not None:\n",
    "        loss = controller.update(update_step, **sample)\n",
    "    \n",
    "    if (update_step % eval_period) == 0:\n",
    "        metrics_list.extend([{**m, 'loss': l.item()} for m, l in zip(off_policy_metrics, loss)])\n",
    "        # We run one batch on policy\n",
    "        metrics_list.extend(\n",
    "            run_batch(env, controller, replay_mem=None, on_policy=True, update_step=update_step))\n",
    "\n",
    "\n",
    "model_file = ...\n",
    "controller.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "id_vars = ['round_number', 'sampling', 'update_step']\n",
    "\n",
    "df = pd.DataFrame.from_records(metrics_list)\n",
    "\n",
    "value_vars = list(set(df.columns) - set(id_vars))\n",
    "\n",
    "df = df.melt(id_vars=id_vars, value_vars=value_vars, var_name='metric')\n",
    "\n",
    "metrics_file = ...\n",
    "\n",
    "df.to_parquet(metrics_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c6aac55b9060d8c5e909fe78b15c7c8c111d482ebd90032ffdeb7e56d867e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
